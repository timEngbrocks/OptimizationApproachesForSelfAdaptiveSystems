\section{Classifying Existing Optimization Approaches}
\label{ch:Existing}

This chapter will classify and compare existing \acrshort{oa} for \acrshort{sas} using the proposed classification.
The comparison can be found in Table \ref{table:ClassifyingExisting}.

\noindent Most of the following existing \acrshort{oa} were found through a literature study conducted by Saputri and Lee \cite*{ApplicationOfMachineLearning}.
The goal of the literature study was to give an overview over the application of machine learning in \acrshort{sas}.
Because of this it is important to acknowledge that they all use some form of machine learning or similar technique to perform their optimizations.
Generally most currently existing use machine learning which is why the conclusions drawn from this comparison should not be influenced by the fact
that they all use machine learning to perform optimizations.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        \textbf{Reference} & \textbf{Location} & \textbf{Purpose} & \textbf{Time} & \textbf{Technique} & \textbf{Approach} & \textbf{\acrshort{rac}} \\
        \hline
        \hline
        \cite*{FUSION} & A & R & D, R & R, K & E - C & M, R \\
        \hline
        \cite*{LearningRevisedModels} & A & R, M & R & R, K & I - C & R \\
        \hline
        \cite*{FIoT} & L & R & R & L, A, R & E - D & R \\
        \hline
        \cite*{ML1} & T & K, U & R & K & I - D & M, U \\
        \hline
        \cite*{ML2} & A, L & U & R & A, L & E - D & U \\
        \hline
        \cite*{ML7} & A & K, U & R & R, K & I - C & M, R, U \\
        \hline
        \cite*{ML8} & A, L & K, R & R, O & R, K & E - C & M, R \\
        \hline
        \cite*{ML11} & A & K, G & D & K & E - C & M, G \\
        \hline
        \cite*{ML12} & A & U & R & R & E - C & R, U \\
        \hline
        \cite*{ML14} & A & U & R & R & I - C & R, U \\
        \hline
    \end{tabular}
    \caption{
        \textbf{Location}: \textbf{A} = Adaptation Control, \textbf{L} = Level, \textbf{T} = Technique \\
        \textbf{Purpose}: \textbf{K} = Update Knowledge, \textbf{R} = Apply Rules / Policies, \\
            \indent\textbf{G} = Goal Satisfaction, \textbf{U} = Utility max-/minimization \\
        \textbf{Time}: \textbf{D} = Design time, \textbf{R} = Run time / Online phase, \textbf{O} = Offline phase \\
        \textbf{Technique}: \textbf{R} = Rules / Policies, \textbf{K} = Knowledge, \\
            \indent\textbf{A} = Adaptation Technique, \textbf{L} = Level \\
        \textbf{Approach}: \textbf{I} = Internal actor, \textbf{E} = External actor, \\
            \indent\textbf{C} = Centralized, \textbf{D} = Decentralized, \textbf{H} = Hybrid \\
        \textbf{\acrshort{rac}}: \textbf{M} = Update Models, \textbf{R} = Modify Rules / Policies, \\
            \indent\textbf{G} = Satisfy Goals, \textbf{U} = Max-/Minimize Utilities
    }
    \label{table:ClassifyingExisting}
\end{table}

\noindent The first conclusion that can be drawn from this comparison of \acrshort{oa} is that most approaches
only use the run time of the system.
This is interesting because most of these approaches use machine learning to perform optimizations as mentioned previously.
Most machine learning algorithms require trainings time before working effectively.
The consequence of directly using an untrained system is that the system will behave in an unexpected or undesired manner in the beginning.
This is especially true for approaches that actively explore the problem space to find optimal solutions like Q-learning \cite*{QLearning}.

\noindent However there is a tradeoff that these \acrshort{oa} have to deal with.
On one hand the system will behave in an unexpected or undesired manner,
on the other hand there is no need for simulating the system to train the \acrshort{oa}.
Simulating the system for training purposes can be difficult as it also requires the simulation of the systems environment.
If the developers of a \acrshort{sas} could predict the environment perfectly,
they would not need an \acrshort{oa} because they could just define adaptation rules and policies that are suited to all ways in which the environment may develop.
This means that training the \acrshort{oa} before the systems runtime can decrease the time that it takes until the system behaves in an expected manner,
but it also can not guarantee that the system will be perfectly trained.

\noindent The second conclusion that can be drawn is that most \acrshort{oa} only search for problems in the Adaptation Control.
But the \textit{Location} dimension offers other possibilities as well.
In addition to the Adaptation Control, the Level or the Technique may require optimizations.

\noindent The third conclusion that can be drawn is that only a few \acrshort{oa} use decentralized approaches.
Using a \acrshort{oa} with a decentralized approach can have the advantage that
each area can be optimized specifically.
As Nascimento and Lucena showed, decentralized approaches can also be used to create self-organizing systems \cite*{FIoT}.
But it also has the disadvantage of increasing the complexity of the \acrshort{oa}.